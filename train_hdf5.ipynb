{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Sequence, Dict, Union, Optional, Callable\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import collections\n",
    "import zarr\n",
    "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
    "from diffusers.training_utils import EMAModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "import pygame\n",
    "import pymunk\n",
    "import pymunk.pygame_util\n",
    "from pymunk.space_debug_draw_options import SpaceDebugColor\n",
    "from pymunk.vec2d import Vec2d\n",
    "import shapely.geometry as sg\n",
    "import cv2\n",
    "import skimage.transform as st\n",
    "from skvideo.io import vwrite\n",
    "from IPython.display import Video\n",
    "import gdown\n",
    "import os\n",
    "\n",
    "\n",
    "# from diffusion_policy.env.pusht.pusht_image_env import PushTImageEnv\n",
    "import imageio \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_indices(\n",
    "        episode_ends:np.ndarray, sequence_length:int,\n",
    "        pad_before: int=0, pad_after: int=0):\n",
    "    indices = list()\n",
    "    for i in range(len(episode_ends)):\n",
    "        start_idx = 0\n",
    "        if i > 0:\n",
    "            start_idx = episode_ends[i-1]\n",
    "        end_idx = episode_ends[i]\n",
    "        episode_length = end_idx - start_idx\n",
    "\n",
    "        min_start = -pad_before\n",
    "        max_start = episode_length - sequence_length + pad_after\n",
    "\n",
    "        # range stops one idx before end\n",
    "        for idx in range(min_start, max_start+1):\n",
    "            buffer_start_idx = max(idx, 0) + start_idx\n",
    "            buffer_end_idx = min(idx+sequence_length, episode_length) + start_idx\n",
    "            start_offset = buffer_start_idx - (idx+start_idx)\n",
    "            end_offset = (idx+sequence_length+start_idx) - buffer_end_idx\n",
    "            sample_start_idx = 0 + start_offset\n",
    "            sample_end_idx = sequence_length - end_offset\n",
    "            indices.append([\n",
    "                buffer_start_idx, buffer_end_idx,\n",
    "                sample_start_idx, sample_end_idx])\n",
    "    indices = np.array(indices)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sequence(train_data, sequence_length,\n",
    "                    buffer_start_idx, buffer_end_idx,\n",
    "                    sample_start_idx, sample_end_idx):\n",
    "    result = dict()\n",
    "    for key, input_arr in train_data.items():\n",
    "        sample = input_arr[buffer_start_idx:buffer_end_idx]\n",
    "        data = sample\n",
    "        if (sample_start_idx > 0) or (sample_end_idx < sequence_length):\n",
    "            data = np.zeros(\n",
    "                shape=(sequence_length,) + input_arr.shape[1:],\n",
    "                dtype=input_arr.dtype)\n",
    "            if sample_start_idx > 0:\n",
    "                data[:sample_start_idx] = sample[0]\n",
    "            if sample_end_idx < sequence_length:\n",
    "                data[sample_end_idx:] = sample[-1]\n",
    "            data[sample_start_idx:sample_end_idx] = sample\n",
    "        result[key] = data\n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data\n",
    "def get_data_stats(data):\n",
    "    data = data.reshape(-1,data.shape[-1])\n",
    "    stats = {\n",
    "        'min': np.min(data, axis=0),\n",
    "        'max': np.max(data, axis=0)\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "def normalize_data(data, stats):\n",
    "    # nomalize to [0,1]\n",
    "    ndata = (data - stats['min']) / (stats['max'] - stats['min'])\n",
    "    # normalize to [-1, 1]\n",
    "    ndata = ndata * 2 - 1\n",
    "    return ndata\n",
    "\n",
    "def unnormalize_data(ndata, stats):\n",
    "    ndata = (ndata + 1) / 2\n",
    "    data = ndata * (stats['max'] - stats['min']) + stats['min']\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datas(hdf5_file, demos):\n",
    "    train_images=[]\n",
    "    train_agent_pos=[]\n",
    "    train_actions=[]\n",
    "\n",
    "    episode_ends=[]\n",
    "    for demo_name in demos: \n",
    "        obs=hdf5_file['data'][demo_name]['obs']\n",
    "        action=hdf5_file['data'][demo_name]['action'][:]\n",
    "\n",
    "        img=obs['img'][:]\n",
    "        agent_pos=obs['agent_pos'][:]\n",
    "\n",
    "        train_images.extend(img)\n",
    "        train_agent_pos.extend(agent_pos)\n",
    "        train_actions.extend(action)\n",
    "\n",
    "        episode_ends.append(len(train_images)) \n",
    "    episode_ends=np.array(episode_ends)\n",
    "    train_images=np.array(train_images)\n",
    "    train_agent_pos=np.array(train_agent_pos)\n",
    "    train_actions=np.array(train_actions)\n",
    "    return train_images, train_agent_pos, train_actions, episode_ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PushTImageDatasetFromHDF5(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 hdf5_file_name: str,\n",
    "                 pred_horizon: int,\n",
    "                 obs_horizon: int,\n",
    "                 action_horizon: int,\n",
    "                 hdf5_filter_key=None):\n",
    "\n",
    "        f=h5py.File(hdf5_file_name, 'r')\n",
    "        if hdf5_filter_key is None:\n",
    "              demos=f['data'].keys()\n",
    "        else:\n",
    "            demos=f['mask'][hdf5_filter_key]\n",
    "            demos=[d.decode('utf-8') for d in demos]\n",
    "\n",
    "        demos = sorted(demos, key=lambda x: int(x.split('_')[1]))\n",
    "\n",
    "        train_images, train_agent_pos, train_actions, episode_ends = get_datas(f, demos)\n",
    "        f.close()\n",
    "\n",
    "        train_image_data = np.moveaxis(train_images, -1,1)\n",
    "        # (N,3,96,96)\n",
    "        train_image_data = train_image_data.astype(np.float32)  #for ns data\n",
    " \n",
    "        # (N, D)\n",
    "        train_data = { \n",
    "            'agent_pos': train_agent_pos,\n",
    "            'action': train_actions\n",
    "        } \n",
    "\n",
    "        # compute start and end of each state-action sequence\n",
    "        # also handles padding\n",
    "        indices = create_sample_indices(\n",
    "            episode_ends=episode_ends,\n",
    "            sequence_length=pred_horizon,\n",
    "            pad_before=obs_horizon-1,\n",
    "            pad_after=action_horizon-1)\n",
    "\n",
    "        # compute statistics and normalized data to [-1,1]\n",
    "        stats = dict()\n",
    "        normalized_train_data = dict()\n",
    "        for key, data in train_data.items():\n",
    "            stats[key] = get_data_stats(data)\n",
    "            normalized_train_data[key] = normalize_data(data, stats[key])\n",
    "\n",
    "        # images are already normalized\n",
    "        normalized_train_data['image'] = train_image_data\n",
    "\n",
    "        self.indices = indices\n",
    "        self.stats = stats\n",
    "        self.normalized_train_data = normalized_train_data\n",
    "        self.pred_horizon = pred_horizon\n",
    "        self.action_horizon = action_horizon\n",
    "        self.obs_horizon = obs_horizon\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get the start/end indices for this datapoint\n",
    "        buffer_start_idx, buffer_end_idx, \\\n",
    "            sample_start_idx, sample_end_idx = self.indices[idx]\n",
    "\n",
    "        # get nomralized data using these indices\n",
    "        nsample = sample_sequence(\n",
    "            train_data=self.normalized_train_data,\n",
    "            sequence_length=self.pred_horizon,\n",
    "            buffer_start_idx=buffer_start_idx,\n",
    "            buffer_end_idx=buffer_end_idx,\n",
    "            sample_start_idx=sample_start_idx,\n",
    "            sample_end_idx=sample_end_idx\n",
    "        )\n",
    "\n",
    "        # discard unused observations\n",
    "        nsample['image'] = nsample['image'][:self.obs_horizon,:]\n",
    "        nsample['agent_pos'] = nsample['agent_pos'][:self.obs_horizon,:]\n",
    "        return nsample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_horizon = 16\n",
    "obs_horizon = 2\n",
    "action_horizon = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file_name='data/pusht/pusht_v7_zarr_206.hdf5'\n",
    "dataset = PushTImageDatasetFromHDF5(\n",
    "    hdf5_file_name=hdf5_file_name,\n",
    "    pred_horizon=pred_horizon,\n",
    "    obs_horizon=obs_horizon,\n",
    "    action_horizon=action_horizon,\n",
    "    hdf5_filter_key=\"f100\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    num_workers=4,\n",
    "    shuffle=True,\n",
    "    # accelerate cpu-gpu transfer\n",
    "    pin_memory=True,\n",
    "    # don't kill worker process afte each epoch\n",
    "    persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 2, 3, 96, 96]),\n",
       " torch.Size([64, 2, 2]),\n",
       " torch.Size([64, 16, 2]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(dataloader))\n",
    "batch['image'].shape, batch['agent_pos'].shape, batch['action'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown ### **Network**\n",
    "#@markdown\n",
    "#@markdown Defines a 1D UNet architecture `ConditionalUnet1D`\n",
    "#@markdown as the noies prediction network\n",
    "#@markdown\n",
    "#@markdown Components\n",
    "#@markdown - `SinusoidalPosEmb` Positional encoding for the diffusion iteration k\n",
    "#@markdown - `Downsample1d` Strided convolution to reduce temporal resolution\n",
    "#@markdown - `Upsample1d` Transposed convolution to increase temporal resolution\n",
    "#@markdown - `Conv1dBlock` Conv1d --> GroupNorm --> Mish\n",
    "#@markdown - `ConditionalResidualBlock1D` Takes two inputs `x` and `cond`. \\\n",
    "#@markdown `x` is passed through 2 `Conv1dBlock` stacked together with residual connection.\n",
    "#@markdown `cond` is applied to `x` with [FiLM](https://arxiv.org/abs/1709.07871) conditioning.\n",
    "\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "class Downsample1d(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(dim, dim, 3, 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class Upsample1d(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose1d(dim, dim, 4, 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Conv1dBlock(nn.Module):\n",
    "    '''\n",
    "        Conv1d --> GroupNorm --> Mish\n",
    "    '''\n",
    "\n",
    "    def __init__(self, inp_channels, out_channels, kernel_size, n_groups=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv1d(inp_channels, out_channels, kernel_size, padding=kernel_size // 2),\n",
    "            nn.GroupNorm(n_groups, out_channels),\n",
    "            nn.Mish(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class ConditionalResidualBlock1D(nn.Module):\n",
    "    def __init__(self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            cond_dim,\n",
    "            kernel_size=3,\n",
    "            n_groups=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Conv1dBlock(in_channels, out_channels, kernel_size, n_groups=n_groups),\n",
    "            Conv1dBlock(out_channels, out_channels, kernel_size, n_groups=n_groups),\n",
    "        ])\n",
    "\n",
    "        # FiLM modulation https://arxiv.org/abs/1709.07871\n",
    "        # predicts per-channel scale and bias\n",
    "        cond_channels = out_channels * 2\n",
    "        self.out_channels = out_channels\n",
    "        self.cond_encoder = nn.Sequential(\n",
    "            nn.Mish(),\n",
    "            nn.Linear(cond_dim, cond_channels),\n",
    "            nn.Unflatten(-1, (-1, 1))\n",
    "        )\n",
    "\n",
    "        # make sure dimensions compatible\n",
    "        self.residual_conv = nn.Conv1d(in_channels, out_channels, 1) \\\n",
    "            if in_channels != out_channels else nn.Identity()\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        '''\n",
    "            x : [ batch_size x in_channels x horizon ]\n",
    "            cond : [ batch_size x cond_dim]\n",
    "\n",
    "            returns:\n",
    "            out : [ batch_size x out_channels x horizon ]\n",
    "        '''\n",
    "        out = self.blocks[0](x)\n",
    "        embed = self.cond_encoder(cond)\n",
    "\n",
    "        embed = embed.reshape(\n",
    "            embed.shape[0], 2, self.out_channels, 1)\n",
    "        scale = embed[:,0,...]\n",
    "        bias = embed[:,1,...]\n",
    "        out = scale * out + bias\n",
    "\n",
    "        out = self.blocks[1](out)\n",
    "        out = out + self.residual_conv(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConditionalUnet1D(nn.Module):\n",
    "    def __init__(self,\n",
    "        input_dim,\n",
    "        global_cond_dim,\n",
    "        diffusion_step_embed_dim=256,\n",
    "        down_dims=[256,512,1024],\n",
    "        kernel_size=5,\n",
    "        n_groups=8\n",
    "        ):\n",
    "        \"\"\"\n",
    "        input_dim: Dim of actions.\n",
    "        global_cond_dim: Dim of global conditioning applied with FiLM\n",
    "          in addition to diffusion step embedding. This is usually obs_horizon * obs_dim\n",
    "        diffusion_step_embed_dim: Size of positional encoding for diffusion iteration k\n",
    "        down_dims: Channel size for each UNet level.\n",
    "          The length of this array determines numebr of levels.\n",
    "        kernel_size: Conv kernel size\n",
    "        n_groups: Number of groups for GroupNorm\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        all_dims = [input_dim] + list(down_dims)\n",
    "        start_dim = down_dims[0]\n",
    "\n",
    "        dsed = diffusion_step_embed_dim\n",
    "        diffusion_step_encoder = nn.Sequential(\n",
    "            SinusoidalPosEmb(dsed),\n",
    "            nn.Linear(dsed, dsed * 4),\n",
    "            nn.Mish(),\n",
    "            nn.Linear(dsed * 4, dsed),\n",
    "        )\n",
    "        cond_dim = dsed + global_cond_dim\n",
    "\n",
    "        in_out = list(zip(all_dims[:-1], all_dims[1:]))\n",
    "        mid_dim = all_dims[-1]\n",
    "        self.mid_modules = nn.ModuleList([\n",
    "            ConditionalResidualBlock1D(\n",
    "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
    "                kernel_size=kernel_size, n_groups=n_groups\n",
    "            ),\n",
    "            ConditionalResidualBlock1D(\n",
    "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
    "                kernel_size=kernel_size, n_groups=n_groups\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "        down_modules = nn.ModuleList([])\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind >= (len(in_out) - 1)\n",
    "            down_modules.append(nn.ModuleList([\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_in, dim_out, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_out, dim_out, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                Downsample1d(dim_out) if not is_last else nn.Identity()\n",
    "            ]))\n",
    "\n",
    "        up_modules = nn.ModuleList([])\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
    "            is_last = ind >= (len(in_out) - 1)\n",
    "            up_modules.append(nn.ModuleList([\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_out*2, dim_in, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_in, dim_in, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                Upsample1d(dim_in) if not is_last else nn.Identity()\n",
    "            ]))\n",
    "\n",
    "        final_conv = nn.Sequential(\n",
    "            Conv1dBlock(start_dim, start_dim, kernel_size=kernel_size),\n",
    "            nn.Conv1d(start_dim, input_dim, 1),\n",
    "        )\n",
    "\n",
    "        self.diffusion_step_encoder = diffusion_step_encoder\n",
    "        self.up_modules = up_modules\n",
    "        self.down_modules = down_modules\n",
    "        self.final_conv = final_conv\n",
    "\n",
    "        print(\"number of parameters: {:e}\".format(\n",
    "            sum(p.numel() for p in self.parameters()))\n",
    "        )\n",
    "\n",
    "    def forward(self,\n",
    "            sample: torch.Tensor,\n",
    "            timestep: Union[torch.Tensor, float, int],\n",
    "            global_cond=None):\n",
    "        \"\"\"\n",
    "        x: (B,T,input_dim)\n",
    "        timestep: (B,) or int, diffusion step\n",
    "        global_cond: (B,global_cond_dim)\n",
    "        output: (B,T,input_dim)\n",
    "        \"\"\"\n",
    "        # (B,T,C)\n",
    "        sample = sample.moveaxis(-1,-2)\n",
    "        # (B,C,T)\n",
    "\n",
    "        # 1. time\n",
    "        timesteps = timestep\n",
    "        if not torch.is_tensor(timesteps):\n",
    "            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n",
    "            timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)\n",
    "        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n",
    "            timesteps = timesteps[None].to(sample.device)\n",
    "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
    "        timesteps = timesteps.expand(sample.shape[0])\n",
    "\n",
    "        global_feature = self.diffusion_step_encoder(timesteps)\n",
    "\n",
    "        if global_cond is not None:\n",
    "            global_feature = torch.cat([\n",
    "                global_feature, global_cond\n",
    "            ], axis=-1)\n",
    "\n",
    "        x = sample\n",
    "        h = []\n",
    "        for idx, (resnet, resnet2, downsample) in enumerate(self.down_modules):\n",
    "            x = resnet(x, global_feature)\n",
    "            x = resnet2(x, global_feature)\n",
    "            h.append(x)\n",
    "            x = downsample(x)\n",
    "\n",
    "        for mid_module in self.mid_modules:\n",
    "            x = mid_module(x, global_feature)\n",
    "\n",
    "        for idx, (resnet, resnet2, upsample) in enumerate(self.up_modules):\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = resnet(x, global_feature)\n",
    "            x = resnet2(x, global_feature)\n",
    "            x = upsample(x)\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "\n",
    "        # (B,C,T)\n",
    "        x = x.moveaxis(-1,-2)\n",
    "        # (B,T,C)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown ### **Vision Encoder**\n",
    "#@markdown\n",
    "#@markdown Defines helper functions:\n",
    "#@markdown - `get_resnet` to initialize standard ResNet vision encoder\n",
    "#@markdown - `replace_bn_with_gn` to replace all BatchNorm layers with GroupNorm\n",
    "\n",
    "def get_resnet(name:str, weights=None, **kwargs) -> nn.Module:\n",
    "    \"\"\"\n",
    "    name: resnet18, resnet34, resnet50\n",
    "    weights: \"IMAGENET1K_V1\", None\n",
    "    \"\"\"\n",
    "    # Use standard ResNet implementation from torchvision\n",
    "    func = getattr(torchvision.models, name)\n",
    "    resnet = func(weights=weights, **kwargs)\n",
    "\n",
    "    # remove the final fully connected layer\n",
    "    # for resnet18, the output dim should be 512\n",
    "    resnet.fc = torch.nn.Identity()\n",
    "    return resnet\n",
    "\n",
    "\n",
    "def replace_submodules(\n",
    "        root_module: nn.Module,\n",
    "        predicate: Callable[[nn.Module], bool],\n",
    "        func: Callable[[nn.Module], nn.Module]) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Replace all submodules selected by the predicate with\n",
    "    the output of func.\n",
    "\n",
    "    predicate: Return true if the module is to be replaced.\n",
    "    func: Return new module to use.\n",
    "    \"\"\"\n",
    "    if predicate(root_module):\n",
    "        return func(root_module)\n",
    "\n",
    "    bn_list = [k.split('.') for k, m\n",
    "        in root_module.named_modules(remove_duplicate=True)\n",
    "        if predicate(m)]\n",
    "    for *parent, k in bn_list:\n",
    "        parent_module = root_module\n",
    "        if len(parent) > 0:\n",
    "            parent_module = root_module.get_submodule('.'.join(parent))\n",
    "        if isinstance(parent_module, nn.Sequential):\n",
    "            src_module = parent_module[int(k)]\n",
    "        else:\n",
    "            src_module = getattr(parent_module, k)\n",
    "        tgt_module = func(src_module)\n",
    "        if isinstance(parent_module, nn.Sequential):\n",
    "            parent_module[int(k)] = tgt_module\n",
    "        else:\n",
    "            setattr(parent_module, k, tgt_module)\n",
    "    # verify that all modules are replaced\n",
    "    bn_list = [k.split('.') for k, m\n",
    "        in root_module.named_modules(remove_duplicate=True)\n",
    "        if predicate(m)]\n",
    "    assert len(bn_list) == 0\n",
    "    return root_module\n",
    "\n",
    "def replace_bn_with_gn(\n",
    "    root_module: nn.Module,\n",
    "    features_per_group: int=16) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Relace all BatchNorm layers with GroupNorm.\n",
    "    \"\"\"\n",
    "    replace_submodules(\n",
    "        root_module=root_module,\n",
    "        predicate=lambda x: isinstance(x, nn.BatchNorm2d),\n",
    "        func=lambda x: nn.GroupNorm(\n",
    "            num_groups=x.num_features//features_per_group,\n",
    "            num_channels=x.num_features)\n",
    "    )\n",
    "    return root_module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.downsample:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += identity\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetFe(nn.Module):\n",
    "    def __init__(self, block, layers):\n",
    "        super(ResNetFe, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False) # (x+2*3-7)/2+1=(x-1)/2+1=48 : 64*48*48\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)       #(x+2*1-3)/2+1=(x-1)/2+1=24  : 64*24*24\n",
    "\n",
    "        # Two ResNet layers\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(128, 512)  # Output 512-dimensional features\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 4:\n",
    "            x = x.unsqueeze(1)\n",
    "        batch_size, seq_len, channels, height, width = x.shape\n",
    "        x = x.view(batch_size * seq_len, channels, height, width)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_encoder = ResNetFe(ResidualBlock, [2, 2]) \n",
    "vision_encoder = replace_bn_with_gn(vision_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 749,120\n"
     ]
    }
   ],
   "source": [
    "n_params= sum(p.numel() for p in vision_encoder.parameters())\n",
    "print(f\"Number of parameters: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 7.994727e+07\n"
     ]
    }
   ],
   "source": [
    " # ResNet18 has output dim of 512\n",
    "vision_feature_dim = 512\n",
    "# agent_pos is 2 dimensional\n",
    "lowdim_obs_dim = 2\n",
    "# observation feature has 514 dims in total per step\n",
    "obs_dim = vision_feature_dim + lowdim_obs_dim\n",
    "action_dim = 2\n",
    "\n",
    "# create network object\n",
    "noise_pred_net = ConditionalUnet1D(\n",
    "    input_dim=action_dim,\n",
    "    global_cond_dim=obs_dim*obs_horizon\n",
    ")\n",
    "\n",
    "# the final arch has 2 parts\n",
    "nets = nn.ModuleDict({\n",
    "    'vision_encoder': vision_encoder,\n",
    "    'noise_pred_net': noise_pred_net\n",
    "})\n",
    "\n",
    "\n",
    "# device transfer\n",
    "device = torch.device('cuda')\n",
    "_ = nets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyScheduler:\n",
    "    def __init__(self, T=100, beta_start=1e-4, beta_end=0.02, device='cuda'):\n",
    "        self.betas=torch.linspace(beta_start, beta_end, T).to(device)\n",
    "        self.T = T\n",
    "        self.alphas=1.0 - self.betas\n",
    "        self.device=device\n",
    "\n",
    "        self.sqrt_one_minus_betas = torch.sqrt(1.0 - self.betas)\n",
    "        \n",
    "        self.alpha_bars = torch.cumprod(self.alphas, dim=0) \n",
    "        \n",
    "        self.sqrt_alpha_bars = torch.sqrt(self.alpha_bars)\n",
    "        self.sqrt_one_minus_alpha_bars = torch.sqrt(1.0 - self.alpha_bars)\n",
    "\n",
    "        self.sqrt_betas = torch.sqrt(self.betas)\n",
    "        self.sqrt_alphas = torch.sqrt(self.alphas)\n",
    "\n",
    "    def extract(self, a, t, x_shape): \n",
    "        \"\"\"\n",
    "        collect the values at time t from a and reshape it to x_shape\n",
    "        \n",
    "        a: precomputed one dimensional values\n",
    "        t: time steps (scalar or tensor)\n",
    "        x_shape: shape of the tensor to be returned \n",
    "        \"\"\"\n",
    "        \n",
    "        b=t.shape[0]\n",
    "        out=a.gather(-1, t)\n",
    "        rt=out.reshape(b, *((1,) * (len(x_shape) - 1)))  #batch, unpack([1]*rest of the dimension)\n",
    "        return rt  \n",
    "\n",
    "    def get_xt(self, x0, t):\n",
    "        \"\"\"\n",
    "        compute noisy image xt from x0 at time t \n",
    "        \"\"\" \n",
    "        eps=torch.randn_like(x0)\n",
    "        # xt=x0 * self.sqrt_alpha_bars[t] + self.sqrt_one_minus_alpha_bars[t] * eps\n",
    "        xt = x0 * self.extract(self.sqrt_alpha_bars, t, x0.shape) + eps * self.extract(self.sqrt_one_minus_alpha_bars, t, x0.shape)\n",
    "        return xt, eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beta_schedule='squaredcos_cap_v2' \n",
    "# # clip output to [-1,1] to improve stability\n",
    "# clip_sample=True\n",
    "\n",
    "num_diffusion_iters = 100\n",
    "noise_scheduler=MyScheduler(T=num_diffusion_iters, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/400 [00:00<?, ?it/s]/home/ns1254/miniforge3/envs/robodiff/lib/python3.9/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "Epoch: 100%|██████████| 400/400 [49:53<00:00,  7.48s/it, loss=0.00822]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 400\n",
    "\n",
    " \n",
    "# Standard ADAM optimizer\n",
    "# Note that EMA parametesr are not optimized\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=nets.parameters(),\n",
    "    lr=1e-4, weight_decay=1e-6)\n",
    "\n",
    "# Cosine LR schedule with linear warmup\n",
    "lr_scheduler = get_scheduler(\n",
    "    name='cosine',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=len(dataloader) * num_epochs\n",
    ")\n",
    "\n",
    "with tqdm(range(num_epochs), desc='Epoch') as tglobal:\n",
    "    # epoch loop\n",
    "    for epoch_idx in tglobal:\n",
    "        # if epoch_idx %10==0:\n",
    "        #     print(\"epoch_idx\",epoch_idx)\n",
    "            \n",
    "        epoch_loss = list()\n",
    "        # batch loop\n",
    "        # with tqdm(dataloader, desc='Batch', leave=False) as tepoch:\n",
    "        for nbatch in dataloader:\n",
    "            # data normalized in dataset\n",
    "            # device transfer\n",
    "            nimage = nbatch['image'][:,:obs_horizon].to(device)\n",
    "            nagent_pos = nbatch['agent_pos'][:,:obs_horizon].to(device)\n",
    "            naction = nbatch['action'].to(device)\n",
    "            B = nagent_pos.shape[0]\n",
    "\n",
    "            # encoder vision features\n",
    "            image_features = nets['vision_encoder'](\n",
    "                nimage.flatten(end_dim=1))\n",
    "            image_features = image_features.reshape(\n",
    "                *nimage.shape[:2],-1)\n",
    "            # (B,obs_horizon,D)\n",
    "\n",
    "            # concatenate vision feature and low-dim obs\n",
    "            obs_features = torch.cat([image_features, nagent_pos], dim=-1)\n",
    "            obs_cond = obs_features.flatten(start_dim=1)\n",
    "            # (B, obs_horizon * obs_dim)\n",
    " \n",
    "            timesteps = torch.randint(\n",
    "                0, noise_scheduler.T,\n",
    "                (B,), device=device\n",
    "            ).long()\n",
    "            noisy_actions , noise= noise_scheduler.get_xt(naction, timesteps)\n",
    "\n",
    "            # predict the noise residual\n",
    "            noise_pred = noise_pred_net(\n",
    "                noisy_actions, timesteps, global_cond=obs_cond)\n",
    "\n",
    "            # L2 loss\n",
    "            loss = nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "            # optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            # step lr scheduler every batch\n",
    "            # this is different from standard pytorch behavior\n",
    "            lr_scheduler.step()\n",
    "\n",
    "\n",
    "            # logging\n",
    "            loss_cpu = loss.item()\n",
    "            epoch_loss.append(loss_cpu)\n",
    "            # tepoch.set_postfix(loss=loss_cpu)\n",
    "        tglobal.set_postfix(loss=np.mean(epoch_loss))\n",
    "\n",
    "# Weights of the EMA model\n",
    "# is used for inference\n",
    "ema_nets = nets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('/home/ns1254/diffusion_policy/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion_policy.env.pusht.pusht_image_env import PushTImageEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PushTImageEnv()\n",
    "\n",
    "nets.eval()\n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDDPM:\n",
    "    def __init__(self, scheduler, noise_predictor_net, device='cuda'): \n",
    "        self.scheduler=scheduler\n",
    "        self.noise_predictor_net=noise_predictor_net\n",
    "        self.device=device\n",
    "    \n",
    "    def forward(self, x0):\n",
    "        \"\"\" \n",
    "        noise x0 at random time t\n",
    "        return noise, predicted noise\n",
    "        \"\"\"\n",
    "        B= x0.shape[0]\n",
    "        t=torch.randint(low=0, high=self.scheduler.T, size=(B,)).long().to(self.device)\n",
    "        xt, eps=self.scheduler.get_xt(x0, t)\n",
    "        eps_pred=self.noise_predictor_net(xt, t)\n",
    "        return xt, eps, eps_pred\n",
    "    \n",
    "    \n",
    "    def x_t_minus_1_from_x_t(self, t, x_t, eps_theta): \n",
    "        \"\"\" \n",
    "        Algorithm 2 in the DDPM paper\n",
    "        \"\"\"\n",
    "        sqrt_alpha=self.scheduler.extract(self.scheduler.sqrt_alphas, t, x_t.shape)\n",
    "        sqrt_one_minus_alpha_bar=self.scheduler.extract(self.scheduler.sqrt_one_minus_alpha_bars, t, x_t.shape)\n",
    "        beta=self.scheduler.extract(self.scheduler.betas, t, x_t.shape)\n",
    "        x_t_minus_1 = (1 / sqrt_alpha) * (x_t - ( beta / sqrt_one_minus_alpha_bar ) * eps_theta)\n",
    "        \n",
    "        # x_t_minus_1 = (1 / self.scheduler.sqrt_alphas[t]) * (x_t - ( self.scheduler.betas[t] / self.scheduler.sqrt_one_minus_alpha_bars[t] ) * eps_theta)\n",
    "        return x_t_minus_1\n",
    "    \n",
    "    def sample_ddpm(self, nsamples, sample_shape, obs_cond):\n",
    "        \"\"\"Sampler following the Denoising Diffusion Probabilistic Models method by Ho et al (Algorithm 2)\"\"\"\n",
    "        with torch.no_grad():\n",
    "            x = torch.randn(size=(nsamples, *sample_shape)).to(self.device)   #start from random noise\n",
    "            xts = [x]\n",
    "            for it in range(self.scheduler.T-1, 0, -1):\n",
    "                t=torch.tensor([it]).repeat_interleave(nsamples, dim=0).long().to(self.device)\n",
    "                # eps_theta = self.noise_predictor_net(x, t) \n",
    "                \n",
    "                eps_theta = self.noise_predictor_net(\n",
    "                    sample=x,\n",
    "                    timestep=t,\n",
    "                    global_cond=obs_cond\n",
    "                ) \n",
    "                \n",
    "                # See DDPM paper between equations 11 and 12\n",
    "                x = self.x_t_minus_1_from_x_t(t, x, eps_theta) \n",
    "                if it > 1: # No noise for t=0\n",
    "                    z = torch.randn(size=(nsamples, *sample_shape)).to(self.device)  \n",
    "                    sqrt_beta=self.scheduler.extract(self.scheduler.sqrt_betas, t, x.shape)       #use fixed varience.\n",
    "                    x += sqrt_beta* z\n",
    "                xts += [x]\n",
    "            return x, xts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpm=MyDDPM(noise_scheduler, nets['noise_pred_net'], device=device)\n",
    "sample_shape=(pred_horizon, action_dim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats=dataset.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(env, nets,  seed, max_steps=200):\n",
    "        \n",
    "    env.seed(200+seed)\n",
    "    obs = env.reset()\n",
    "\n",
    "    # keep a queue of last 2 steps of observations\n",
    "    obs_deque = collections.deque(\n",
    "        [obs] * obs_horizon, maxlen=obs_horizon)\n",
    "    # save visualization and rewards\n",
    "    imgs = [env.render(mode='rgb_array')]\n",
    "    rewards = list()\n",
    "    done = False\n",
    "    step_idx = 0\n",
    "    success=False\n",
    "    with tqdm(total=max_steps, desc=\"Eval PushTImageEnv\") as pbar:\n",
    "        while not done:\n",
    "            B = 1\n",
    "            # stack the last obs_horizon number of observations\n",
    "            images = np.stack([x['image'] for x in obs_deque])\n",
    "            agent_poses = np.stack([x['agent_pos'] for x in obs_deque])\n",
    "\n",
    "            # normalize observation\n",
    "            nagent_poses = normalize_data(agent_poses, stats=stats['agent_pos'])\n",
    "            # images are already normalized to [0,1]\n",
    "            nimages = images\n",
    "\n",
    "            # device transfer\n",
    "            nimages = torch.from_numpy(nimages).to(device, dtype=torch.float32)\n",
    "            # (2,3,96,96)\n",
    "            nagent_poses = torch.from_numpy(nagent_poses).to(device, dtype=torch.float32)\n",
    "            # (2,2)\n",
    "\n",
    "            # infer action\n",
    "            with torch.no_grad(): \n",
    "                image_features = nets['vision_encoder'](nimages)\n",
    "                # (2,512) \n",
    "                # concat with low-dim observations\n",
    "                obs_features = torch.cat([image_features, nagent_poses], dim=-1) \n",
    "                # reshape observation to (B,obs_horizon*obs_dim)\n",
    "                obs_cond = obs_features.unsqueeze(0).flatten(start_dim=1)\n",
    "\n",
    "                naction,xts=ddpm.sample_ddpm(1, sample_shape, obs_cond)\n",
    "                \n",
    "\n",
    "            # unnormalize action\n",
    "            naction = naction.detach().to('cpu').numpy()\n",
    "            # (B, pred_horizon, action_dim)\n",
    "            naction = naction[0]\n",
    "            action_pred = unnormalize_data(naction, stats=stats['action'])\n",
    "\n",
    "            # only take action_horizon number of actions\n",
    "            start = obs_horizon - 1\n",
    "            end = start + action_horizon\n",
    "            action = action_pred[start:end,:]\n",
    "            # (action_horizon, action_dim)\n",
    "\n",
    "            # execute action_horizon number of steps\n",
    "            # without replanning\n",
    "            for i in range(len(action)):\n",
    "                obs, reward, done, info = env.step(action[i])\n",
    "                obs_deque.append(obs)\n",
    "                \n",
    "                rewards.append(reward)\n",
    "                imgs.append(env.render(mode='rgb_array'))\n",
    "\n",
    "                # update progress bar\n",
    "                step_idx += 1\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix(reward=reward)\n",
    "                if step_idx > max_steps:\n",
    "                    done = True\n",
    "                if done:\n",
    "                    success=True\n",
    "                    break\n",
    "\n",
    "    return max(rewards) , success, imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval PushTImageEnv: 201it [00:13, 15.41it/s, reward=0.452]                           \n",
      "Eval PushTImageEnv: 201it [00:13, 15.40it/s, reward=0.978]                           \n",
      "Eval PushTImageEnv: 201it [00:13, 15.39it/s, reward=0]                               \n",
      "Eval PushTImageEnv:  78%|███████▊  | 157/200 [00:10<00:02, 15.61it/s, reward=1]    \n",
      "Eval PushTImageEnv:  95%|█████████▌| 190/200 [00:12<00:00, 15.71it/s, reward=1]      \n",
      "Eval PushTImageEnv: 201it [00:13, 15.40it/s, reward=0.268]                          \n",
      "Eval PushTImageEnv: 201it [00:13, 15.40it/s, reward=0.708]                           \n",
      "Eval PushTImageEnv: 201it [00:13, 15.40it/s, reward=0.119]                            \n",
      "Eval PushTImageEnv: 201it [00:13, 15.39it/s, reward=0.486]                         \n",
      "Eval PushTImageEnv:  56%|█████▋    | 113/200 [00:07<00:05, 15.00it/s, reward=1]      \n",
      "Eval PushTImageEnv: 201it [00:13, 15.39it/s, reward=0]                              \n",
      "Eval PushTImageEnv: 201it [00:13, 15.41it/s, reward=0.955]                           \n",
      "Eval PushTImageEnv: 201it [00:13, 15.41it/s, reward=0]                         \n",
      "Eval PushTImageEnv: 201it [00:13, 15.39it/s, reward=0.632]                         \n",
      "Eval PushTImageEnv: 201it [00:13, 15.38it/s, reward=0.778]                          \n",
      "Eval PushTImageEnv: 201it [00:13, 15.38it/s, reward=0.995]                          \n",
      "Eval PushTImageEnv: 201it [00:13, 15.36it/s, reward=0.375]                         \n",
      "Eval PushTImageEnv:  86%|████████▌ | 172/200 [00:11<00:01, 15.54it/s, reward=1]      \n",
      "Eval PushTImageEnv: 201it [00:13, 15.38it/s, reward=0.734]                           \n",
      "Eval PushTImageEnv:  71%|███████   | 142/200 [00:09<00:03, 15.67it/s, reward=1]     \n",
      "Eval PushTImageEnv: 201it [00:13, 15.39it/s, reward=0]                         \n",
      "Eval PushTImageEnv: 201it [00:13, 15.38it/s, reward=0.707]                           \n",
      "Eval PushTImageEnv:  46%|████▌     | 92/200 [00:06<00:07, 15.21it/s, reward=1]      \n",
      "Eval PushTImageEnv: 201it [00:13, 15.37it/s, reward=0.157]                         \n",
      "Eval PushTImageEnv: 201it [00:13, 15.37it/s, reward=0.261]                         \n",
      "Eval PushTImageEnv: 201it [00:13, 15.35it/s, reward=0.896]                           \n",
      "Eval PushTImageEnv:  48%|████▊     | 96/200 [00:06<00:06, 15.81it/s, reward=1]    \n",
      "Eval PushTImageEnv: 201it [00:13, 15.38it/s, reward=0.592]                          \n",
      "Eval PushTImageEnv: 201it [00:13, 15.38it/s, reward=0.76]                          \n",
      "Eval PushTImageEnv: 201it [00:13, 15.40it/s, reward=0]                         \n",
      "Eval PushTImageEnv: 201it [00:13, 15.41it/s, reward=0.999]                         \n",
      "Eval PushTImageEnv: 201it [00:13, 15.39it/s, reward=1]                               \n",
      "Eval PushTImageEnv: 201it [00:13, 15.39it/s, reward=0.5]                            \n",
      "Eval PushTImageEnv: 201it [00:13, 15.38it/s, reward=0.442]                         \n",
      "Eval PushTImageEnv: 201it [00:13, 15.35it/s, reward=0.173]                         \n",
      "Eval PushTImageEnv: 201it [00:13, 15.40it/s, reward=0]                         \n",
      "Eval PushTImageEnv: 201it [00:13, 15.39it/s, reward=0.845]                         \n",
      "Eval PushTImageEnv: 201it [00:13, 15.37it/s, reward=0.593]                          \n",
      "Eval PushTImageEnv: 201it [00:13, 15.36it/s, reward=0.707]                         \n",
      "Eval PushTImageEnv: 201it [00:13, 15.39it/s, reward=0.965]                           \n",
      "Eval PushTImageEnv: 201it [00:13, 15.35it/s, reward=0.261]                           \n",
      "Eval PushTImageEnv:  88%|████████▊ | 177/200 [00:11<00:01, 15.30it/s, reward=1]    \n",
      "Eval PushTImageEnv: 100%|█████████▉| 199/200 [00:12<00:00, 15.81it/s, reward=1]    \n",
      "Eval PushTImageEnv:  58%|█████▊    | 117/200 [00:07<00:05, 15.56it/s, reward=1]      \n",
      "Eval PushTImageEnv: 201it [00:13, 15.44it/s, reward=0]                               \n",
      "Eval PushTImageEnv: 201it [00:13, 15.44it/s, reward=0.944]                         \n",
      "Eval PushTImageEnv: 201it [00:13, 15.44it/s, reward=0.923]                         \n",
      "Eval PushTImageEnv: 201it [00:13, 15.45it/s, reward=0]                         \n",
      "Eval PushTImageEnv: 201it [00:13, 15.44it/s, reward=0]                              \n",
      "Eval PushTImageEnv: 201it [00:13, 15.34it/s, reward=0]                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward:  0.6404698604048779\n",
      "Success Rate:  1.0\n",
      "Mean Length:  190.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rewards=[]\n",
    "success=[]\n",
    "lengths=[]\n",
    "seed=40\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "for i in range(50):\n",
    "    reward, suc, imgs = rollout(env, nets, seed+i, 200)\n",
    "    rewards.append(reward)\n",
    "    success.append(suc)\n",
    "    lengths.append(len(imgs))\n",
    "\n",
    "print('Mean Reward: ', np.mean(rewards))\n",
    "print('Success Rate: ', np.mean(success))\n",
    "print('Mean Length: ', np.mean(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
