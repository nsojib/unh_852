{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ns1254/miniforge3/envs/robodiff/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.16, Python 3.9.18)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ns1254/miniforge3/envs/robodiff/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple, Sequence, Dict, Union, Optional, Callable\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from diffusers.optimization import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "import pygame\n",
    "import pymunk\n",
    "import pymunk.pygame_util\n",
    "from pymunk.space_debug_draw_options import SpaceDebugColor\n",
    "from pymunk.vec2d import Vec2d\n",
    "import shapely.geometry as sg\n",
    "import cv2\n",
    "import skimage.transform as st\n",
    "from skvideo.io import vwrite\n",
    "from IPython.display import Video\n",
    "import gdown\n",
    "import os\n",
    "\n",
    "\n",
    "# from diffusion_policy.env.pusht.pusht_image_env import PushTImageEnv\n",
    "import imageio \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import h5py\n",
    "\n",
    "from pusht_data_utils import get_data_stats, normalize_data, unnormalize_data, PushTImageDatasetFromHDF5\n",
    "from vision_model import ResidualBlock, ResNetFe, replace_bn_with_gn\n",
    "from noise_predictor_model import ConditionalUnet1D\n",
    "from myddpm import MyScheduler, MyDDPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_horizon = 16\n",
    "obs_horizon = 2\n",
    "action_horizon = 8\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file_name='data/pusht/pusht_v7_zarr_206.hdf5'\n",
    "dataset = PushTImageDatasetFromHDF5(\n",
    "    hdf5_file_name=hdf5_file_name,\n",
    "    pred_horizon=pred_horizon,\n",
    "    obs_horizon=obs_horizon,\n",
    "    action_horizon=action_horizon,\n",
    "    hdf5_filter_key=\"f50\"\n",
    ")\n",
    "stats=dataset.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 2, 3, 96, 96]),\n",
       " torch.Size([64, 2, 2]),\n",
       " torch.Size([64, 16, 2]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    num_workers=4,\n",
    "    shuffle=True,\n",
    "    # accelerate cpu-gpu transfer\n",
    "    pin_memory=True,\n",
    "    # don't kill worker process afte each epoch\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "batch = next(iter(dataloader))\n",
    "batch['image'].shape, batch['agent_pos'].shape, batch['action'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_encoder = ResNetFe(ResidualBlock, [2, 2]) \n",
    "vision_encoder = replace_bn_with_gn(vision_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConditionalUnet1D: number of parameters: 7.994727e+07\n"
     ]
    }
   ],
   "source": [
    "vision_feature_dim = 512\n",
    "lowdim_obs_dim = 2\n",
    "\n",
    "obs_dim = vision_feature_dim + lowdim_obs_dim\n",
    "action_dim = 2\n",
    "\n",
    "noise_pred_net = ConditionalUnet1D(\n",
    "    input_dim=action_dim,\n",
    "    global_cond_dim=obs_dim*obs_horizon\n",
    ")\n",
    "\n",
    "nets = nn.ModuleDict({\n",
    "    'vision_encoder': vision_encoder,\n",
    "    'noise_pred_net': noise_pred_net\n",
    "})\n",
    "\n",
    "_ = nets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_diffusion_iters = 100\n",
    "sample_shape=(pred_horizon, action_dim) \n",
    "\n",
    "noise_scheduler=MyScheduler(T=num_diffusion_iters, device=device)\n",
    "ddpm=MyDDPM(noise_scheduler, nets['noise_pred_net'], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/400 [00:00<?, ?it/s]/home/ns1254/miniforge3/envs/robodiff/lib/python3.9/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "Epoch: 100%|█████████▉| 399/400 [25:48<00:03,  3.87s/it, loss=0.00815]"
     ]
    }
   ],
   "source": [
    "num_epochs = 400\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=nets.parameters(),\n",
    "    lr=1e-4, weight_decay=1e-6)\n",
    "\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name='cosine',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=len(dataloader) * num_epochs\n",
    ")\n",
    "\n",
    "with tqdm(range(num_epochs), desc='Epoch') as tglobal:\n",
    "    \n",
    "    for epoch_idx in tglobal:\n",
    "            \n",
    "        epoch_loss = list()\n",
    "        for nbatch in dataloader:\n",
    "            \n",
    "            nimage = nbatch['image'][:,:obs_horizon].to(device)\n",
    "            nagent_pos = nbatch['agent_pos'][:,:obs_horizon].to(device)\n",
    "            naction = nbatch['action'].to(device)\n",
    "            B = nagent_pos.shape[0]\n",
    "\n",
    "            \n",
    "            image_features = nets['vision_encoder'](\n",
    "                nimage.flatten(end_dim=1))\n",
    "            image_features = image_features.reshape(\n",
    "                *nimage.shape[:2],-1)\n",
    "            # (B,obs_horizon,D)\n",
    "\n",
    "            # concatenate vision feature and low-dim obs\n",
    "            obs_features = torch.cat([image_features, nagent_pos], dim=-1)\n",
    "            obs_cond = obs_features.flatten(start_dim=1)\n",
    "            # (B, obs_horizon * obs_dim)\n",
    " \n",
    "            timesteps = torch.randint(\n",
    "                0, noise_scheduler.T,\n",
    "                (B,), device=device\n",
    "            ).long()\n",
    "            noisy_actions , eps= noise_scheduler.get_xt(naction, timesteps)\n",
    "\n",
    "            eps_theta = noise_pred_net(noisy_actions, timesteps, global_cond=obs_cond)\n",
    "\n",
    "            loss = nn.functional.mse_loss(eps_theta, eps)\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            # step lr scheduler every batch\n",
    "            # this is different from standard pytorch behavior\n",
    "            lr_scheduler.step()\n",
    "\n",
    "\n",
    "            loss_cpu = loss.item()\n",
    "            epoch_loss.append(loss_cpu)\n",
    "           \n",
    "        tglobal.set_postfix(loss=np.mean(epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('/home/ns1254/diffusion_policy/')\n",
    "\n",
    "from diffusion_policy.env.pusht.pusht_image_env import PushTImageEnv\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PushTImageEnv()\n",
    "\n",
    "nets.eval()\n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(env, nets,  seed, max_steps=200):\n",
    "        \n",
    "    env.seed(200+seed)\n",
    "    obs = env.reset()\n",
    "\n",
    "    # keep a queue of last 2 steps of observations\n",
    "    obs_deque = collections.deque(\n",
    "        [obs] * obs_horizon, maxlen=obs_horizon)\n",
    "    # save visualization and rewards\n",
    "    imgs = [env.render(mode='rgb_array')]\n",
    "    rewards = list()\n",
    "    done = False\n",
    "    step_idx = 0\n",
    "    success=False\n",
    "    with tqdm(total=max_steps, desc=\"Eval PushTImageEnv\") as pbar:\n",
    "        while not done:\n",
    "            B = 1\n",
    "            # stack the last obs_horizon number of observations\n",
    "            images = np.stack([x['image'] for x in obs_deque])\n",
    "            agent_poses = np.stack([x['agent_pos'] for x in obs_deque])\n",
    "\n",
    "            # normalize observation\n",
    "            nagent_poses = normalize_data(agent_poses, stats=stats['agent_pos'])\n",
    "            # images are already normalized to [0,1]\n",
    "            nimages = images\n",
    "\n",
    "            # device transfer\n",
    "            nimages = torch.from_numpy(nimages).to(device, dtype=torch.float32)\n",
    "            # (2,3,96,96)\n",
    "            nagent_poses = torch.from_numpy(nagent_poses).to(device, dtype=torch.float32)\n",
    "            # (2,2)\n",
    "\n",
    "            # infer action\n",
    "            with torch.no_grad(): \n",
    "                image_features = nets['vision_encoder'](nimages)\n",
    "                # (2,512) \n",
    "                # concat with low-dim observations\n",
    "                obs_features = torch.cat([image_features, nagent_poses], dim=-1) \n",
    "                # reshape observation to (B,obs_horizon*obs_dim)\n",
    "                obs_cond = obs_features.unsqueeze(0).flatten(start_dim=1)\n",
    "\n",
    "                naction,xts=ddpm.sample_ddpm(1, sample_shape, obs_cond)\n",
    "                \n",
    "\n",
    "            # unnormalize action\n",
    "            naction = naction.detach().to('cpu').numpy()\n",
    "            # (B, pred_horizon, action_dim)\n",
    "            naction = naction[0]\n",
    "            action_pred = unnormalize_data(naction, stats=stats['action'])\n",
    "\n",
    "            # only take action_horizon number of actions\n",
    "            start = obs_horizon - 1\n",
    "            end = start + action_horizon\n",
    "            action = action_pred[start:end,:]\n",
    "            # (action_horizon, action_dim)\n",
    "\n",
    "            # execute action_horizon number of steps\n",
    "            # without replanning\n",
    "            for i in range(len(action)):\n",
    "                obs, reward, done, info = env.step(action[i])\n",
    "                obs_deque.append(obs)\n",
    "                \n",
    "                rewards.append(reward)\n",
    "                imgs.append(env.render(mode='rgb_array'))\n",
    "\n",
    "                # update progress bar\n",
    "                step_idx += 1\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix(reward=reward)\n",
    "                if step_idx > max_steps:\n",
    "                    done = True\n",
    "                if done:\n",
    "                    success=True\n",
    "                    break\n",
    "\n",
    "    return max(rewards) , success, imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards=[]\n",
    "success=[]\n",
    "lengths=[]\n",
    "seed=40\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "for i in range(50):\n",
    "    reward, suc, imgs = rollout(env, nets, seed+i, 200)\n",
    "    rewards.append(reward)\n",
    "    success.append(suc)\n",
    "    lengths.append(len(imgs))\n",
    "\n",
    "print('Mean Reward: ', np.mean(rewards))\n",
    "print('Success Rate: ', np.mean(success))\n",
    "print('Mean Length: ', np.mean(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
